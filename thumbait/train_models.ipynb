{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/patryk/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/patryk/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/patryk/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from gensim.models import KeyedVectors\n",
    "import fasttext\n",
    "from matplotlib import pyplot as plt\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "\n",
    "from tqdm import tqdm\n",
    "from typing import Union, Dict, Tuple, Optional\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch\n",
    "import torchvision as tv\n",
    "import torchvision.models as M\n",
    "import torchvision.transforms as T\n",
    "import cv2\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "DATA_PATH = f\"data/\"\n",
    "SEED = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "548866548.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(f\"{DATA_PATH}merged_data_youtube.csv\", index_col=0)\n",
    "df_view_max = df[\"view_count\"].max()\n",
    "df_view_min = df[\"view_count\"].min()\n",
    "\n",
    "df[\"view_count\"] = ((df[\"view_count\"]-df_view_min)/(df_view_max-df_view_min)).astype(np.float32) # Normalize view count\n",
    "df[\"likes\"] = ((df[\"likes\"]-df[\"likes\"].min())/(df[\"likes\"].max()-df[\"likes\"].min())).astype(np.float32) # Normalize likes\n",
    "df[\"comment_count\"] = ((df[\"comment_count\"]-df[\"comment_count\"].min())/(df[\"comment_count\"].max()-df[\"comment_count\"].min())).astype(np.float32) # Normalize comment count\n",
    "# df = df.dropna()\n",
    "print(df_view_max)\n",
    "print(df_view_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"view_count\"] = df[\"view_count\"].fillna(0)\n",
    "df[\"likes\"] = df[\"likes\"].fillna(0)\n",
    "df[\"comment_count\"] = df[\"comment_count\"].fillna(0)\n",
    "\n",
    "df[\"title\"] = df[\"title\"].fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "en_stop = stopwords.words('english')\n",
    "\n",
    "def preprocess_text(document, stemmer):\n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r\"\\W\", \" \", str(document))\n",
    "\n",
    "    # remove all single characters\n",
    "    document = re.sub(r\"\\s+[a-zA-Z]\\s+\", \" \", document)\n",
    "\n",
    "    # Remove single characters from the start\n",
    "    document = re.sub(r\"\\^[a-zA-Z]\\s+\", \" \", document)\n",
    "\n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r\"\\s+\", \" \", document, flags=re.I)\n",
    "\n",
    "    # Removing prefixed 'b'\n",
    "    document = re.sub(r\"^b\\s+\", \"\", document)\n",
    "\n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "\n",
    "    # Lemmatization\n",
    "    tokens = document.split()\n",
    "    tokens = [stemmer.lemmatize(word) for word in tokens]\n",
    "    tokens = [word for word in tokens if word not in en_stop]\n",
    "    tokens = [word for word in tokens if len(word) > 3]\n",
    "\n",
    "    # preprocessed_text = ' '.join(tokens)\n",
    "    preprocessed_text = tokens\n",
    "\n",
    "    return preprocessed_text\n",
    "\n",
    "def pad_collate(batch):\n",
    "    (texts, images, labels) = zip(*batch)\n",
    "    texts = [text if len(text) else torch.zeros(1, 300) for text in texts]\n",
    "    text_lens = [len(text) for text in texts]\n",
    "    texts_pad = pad_sequence(texts, batch_first=True, padding_value=0)\n",
    "    texts_pad = pack_padded_sequence(\n",
    "        texts_pad, text_lens, batch_first=True, enforce_sorted=False\n",
    "    )\n",
    "    return texts_pad, images, torch.tensor(labels)\n",
    "\n",
    "\n",
    "def strip_n_collate(batch):\n",
    "    strip_n: int = 40\n",
    "    (texts, summaries, labels) = zip(*batch)\n",
    "    text_lens = [len(x) for x in texts]\n",
    "    summary_lens = [len(x) for x in summaries]\n",
    "\n",
    "    texts_pad = pad_sequence(texts, batch_first=True, padding_value=0)\n",
    "    summaries_pad = pad_sequence(summaries, batch_first=True, padding_value=0)\n",
    "\n",
    "    texts_pad = [item[:40, :].unsqueeze(dim=0) for item in texts_pad]\n",
    "    summaries_pad = [item[:40, :].unsqueeze(dim=0) for item in summaries_pad]\n",
    "\n",
    "    texts_pad = torch.cat(texts_pad)\n",
    "    summaries_pad = torch.cat(summaries_pad)\n",
    "\n",
    "    # texts_pad = pack_padded_sequence(\n",
    "    #     texts_pad, text_lens, batch_first=True, enforce_sorted=False\n",
    "    # )\n",
    "    # summaries_pad = pack_padded_sequence(\n",
    "    #     summaries_pad, summary_lens, batch_first=True, enforce_sorted=False\n",
    "    # )\n",
    "\n",
    "    return texts_pad, summaries_pad, torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, kwargs):\n",
    "        super(LSTM, self).__init__()\n",
    "    \n",
    "        self.num_classes = kwargs.get(\"num_classes\", 1)\n",
    "        self.input_size = kwargs.get(\"input_size\", 100)\n",
    "        self.hidden_size = kwargs.get(\"hidden_size\", 64)\n",
    "        self.num_layers = kwargs.get(\"num_layers\", 2)\n",
    "        self.image_data_loader = kwargs.get(\"image_data_loader\")\n",
    "\n",
    "        self.resnet = M.resnet50(pretrained=False)\n",
    "        self.resnet.fc = torch.nn.Identity()\n",
    "\n",
    "        self.lstm_title = nn.LSTM(\n",
    "            input_size=self.input_size,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=True,\n",
    "        )  # lstm\n",
    "\n",
    "        self.fc_1 = nn.Linear(2048 + self.hidden_size, self.hidden_size)  # fully connected 1\n",
    "        self.fc = nn.Linear(self.hidden_size, self.num_classes)  # fully connected last layer\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, text, image):\n",
    "        # Propagate input through LSTM\n",
    "        output_title, (hn_title, cn_title) = self.lstm_title(\n",
    "            text\n",
    "        )  # lstm with input, hidden, and internal state\n",
    "\n",
    "        hn_title = hn_title[-1].view(\n",
    "            -1, self.hidden_size\n",
    "        )  # reshaping the data for Dense layer next\n",
    "        imgs = torch.cat([self.image_data_loader.dataset[img][0].unsqueeze(dim=0) for img in image]).cuda()\n",
    "\n",
    "        print(imgs.shape)\n",
    "\n",
    "        out_img = self.resnet(imgs)\n",
    "        out = torch.cat((hn_title, out_img), axis=1)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc_1(out)  # first Dense\n",
    "        out = self.relu(out)  # relu\n",
    "        out = self.fc(out)  # Final Output\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, r2_score\n",
    "\n",
    "\n",
    "def _ensure_exists(path_out: str) -> None:\n",
    "    if os.path.exists(path_out):\n",
    "        return\n",
    "    os.makedirs(path_out)\n",
    "\n",
    "\n",
    "def count_correct(y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
    "    preds = torch.argmax(y_pred, dim=1)\n",
    "    return (preds == y_true).float().sum()\n",
    "\n",
    "\n",
    "def calc_fscore(y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
    "    preds = torch.argmax(y_pred, dim=1)\n",
    "    return f1_score(y_true, preds, average=\"macro\")\n",
    "\n",
    "def calc_r2score(y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
    "    return r2_score(y_true.detach().numpy(), y_pred.detach().numpy(), multioutput='variance_weighted')\n",
    "\n",
    "def calc_precission(y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
    "    preds = torch.argmax(y_pred, dim=1)\n",
    "    return precision_score(y_true, preds, average=\"macro\", labels=np.unique(preds))\n",
    "\n",
    "\n",
    "def validate(\n",
    "        model: nn.Module, loss_fn: torch.nn.CrossEntropyLoss, dataloader: DataLoader\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    loss = 0\n",
    "    r2_score_sum = 0\n",
    "    _all = 0\n",
    "    iters = 0\n",
    "    for X_1_batch, X_2_batch, y_batch in dataloader:\n",
    "        y_pred = model(X_1_batch.cuda(), X_2_batch)\n",
    "        _all += len(y_pred)\n",
    "        iters += 1\n",
    "        loss += loss_fn(y_pred.flatten(), y_batch.cuda())\n",
    "        r2_score_sum += r2_score(y_pred.cpu().flatten(), y_batch)\n",
    "    return loss / _all, r2_score_sum / iters\n",
    "\n",
    "\n",
    "def fit(\n",
    "        model: nn.Module,\n",
    "        optimizer: optim.Optimizer,\n",
    "        loss_fn: torch.nn.CrossEntropyLoss,\n",
    "        train_dl: DataLoader,\n",
    "        val_dl: DataLoader,\n",
    "        writer: SummaryWriter,\n",
    "        test_dl: Union[None, DataLoader] = None,\n",
    "        epochs: int = 50,\n",
    "        print_metrics: bool = True,\n",
    "        patience: int = 5,\n",
    "        output_path: str = \"data/checkpoints/best\",\n",
    "        run_prefix: str = \"early_stopping\",\n",
    ") -> Dict[str, list]:\n",
    "    losses = {\"train\": [], \"val\": [], \"test\": []}\n",
    "    r_2s = {\"train\": [], \"val\": [], \"test\": []}\n",
    "\n",
    "    min_val_loss = 1e10\n",
    "    current_patience = 0\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        model.train()  # Przełączenie na tryb uczenia modelu - istotne dla takich warstw jak Dropuot czy BatchNorm\n",
    "        for X_1_batch, X_2_batch, y_batch in train_dl:\n",
    "            X_1_batch, X_2_batch, y_batch = (\n",
    "                X_1_batch.cuda(),\n",
    "                X_2_batch,\n",
    "                y_batch.cuda(),\n",
    "            )\n",
    "            y_pred = model(\n",
    "                X_1_batch, X_2_batch\n",
    "            )  # Uzyskanie pseudoprawdopodobieństw dla próbek z minibatcha\n",
    "            loss = loss_fn(y_pred.flatten(), y_batch)  # Policzenie funkcji straty\n",
    "            \n",
    "            loss.backward()  # Wsteczna propagacja z wyniku funkcji straty - policzenie gradientów i zapisanie ich w tensorach (parametrach)\n",
    "            optimizer.step()  # Aktualizacja parametrów modelu przez optymalizator na podstawie gradientów zapisanych w tensorach (parametrach) oraz lr\n",
    "            optimizer.zero_grad()  # Wyzerowanie gradientów w modelu, alternatywnie można wywołać model.zero_grad()\n",
    "\n",
    "        model.eval()  # Przełączenie na tryb ewaluacji modelu - istotne dla takich warstw jak Dropuot czy BatchNorm\n",
    "        with torch.no_grad():  # Wstrzymujemy przeliczanie i śledzenie gradientów dla tensorów - w procesie ewaluacji modelu nie chcemy zmian w gradientach\n",
    "            train_loss, train_r2 = validate(\n",
    "                model, loss_fn, train_dl\n",
    "            )\n",
    "            val_loss, val_r2 = validate(model, loss_fn, val_dl)\n",
    "\n",
    "            if val_loss < min_val_loss:\n",
    "                min_val_loss = val_loss\n",
    "                current_patience = 0\n",
    "                torch.save(\n",
    "                    obj={\n",
    "                        \"epoch\": epoch,\n",
    "                        \"model_state_dict\": model.state_dict(),\n",
    "                        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                    },\n",
    "                    f=output_path + \"_\" + run_prefix,\n",
    "                )\n",
    "            else:\n",
    "                current_patience += 1\n",
    "\n",
    "            if test_dl:\n",
    "                test_loss, test_r2 = validate(model, loss_fn, test_dl)\n",
    "                losses[\"test\"].append(test_loss)\n",
    "                r_2s[\"test\"].append(test_r2)\n",
    "\n",
    "        losses[\"train\"].append(train_loss)\n",
    "        losses[\"val\"].append(val_loss)\n",
    "\n",
    "        r_2s[\"train\"].append(train_r2)\n",
    "        r_2s[\"val\"].append(val_r2)\n",
    "\n",
    "        writer.add_scalars(\n",
    "            main_tag=f\"{run_prefix} loss\",\n",
    "            tag_scalar_dict={\"train\": train_loss, \"dev\": val_loss},\n",
    "            global_step=epoch + 1,\n",
    "        )\n",
    "\n",
    "        writer.add_scalars(\n",
    "            main_tag=f\"{run_prefix} r_2 score\",\n",
    "            tag_scalar_dict={\"train\": train_r2, \"dev\": val_r2},\n",
    "            global_step=epoch + 1,\n",
    "        )\n",
    "\n",
    "        if print_metrics:\n",
    "            print(\n",
    "                f\"Epoch {epoch}: \"\n",
    "                f\"train loss = {train_loss:.3f} (r2: {train_r2:.3f})\"\n",
    "                f\"validation loss = {val_loss:.3f} (r2: {val_r2:.3f}))\"\n",
    "            )\n",
    "\n",
    "        if current_patience >= patience:\n",
    "            break\n",
    "\n",
    "    model.eval()  # Przełączenie na tryb ewaluacji modelu - istotne dla takich warstw jak Dropuot czy BatchNorm\n",
    "    return losses, r_2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TitleDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            all_data,\n",
    "            model_name: str,\n",
    "            train: bool,\n",
    "            data_path: str,\n",
    "            model = None,\n",
    "            k: int = 10,\n",
    "            n_start: int = 0,\n",
    "            prefix: str = \"train\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        k - k is k letter in 'k-crossvalidation'\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model = model\n",
    "        self.all_data_len = len(all_data)\n",
    "        self.prefix = prefix\n",
    "        self.all_data_len = len(all_data)\n",
    "        self.all_data = all_data\n",
    "        self.k = k\n",
    "        self.train = train\n",
    "        self.splits = []\n",
    "        self.data = []\n",
    "        self.prepare_splits(n_start)\n",
    "        # self.data_path = os.path.join(data_path, model_name, self.prefix)\n",
    "        self.data_path = os.path.join(data_path, model_name, self.prefix)\n",
    "        self.stemmer = WordNetLemmatizer()\n",
    "\n",
    "    def prepare_splits(self, n):\n",
    "        self.splits = [\n",
    "            list(range(len(self.all_data)))[\n",
    "            i\n",
    "            * len(self.all_data)\n",
    "            // self.k : (i + 1)\n",
    "                        * len(self.all_data)\n",
    "                        // self.k\n",
    "            ]\n",
    "            if i != self.k - 1\n",
    "            else list(range(len(self.all_data)))[i * len(self.all_data) // self.k :]\n",
    "            for i in range(self.k)\n",
    "        ]\n",
    "        if self.train:\n",
    "            self.data = [\n",
    "                item for p, split in enumerate(self.splits) if p != n for item in split\n",
    "            ]\n",
    "        else:\n",
    "            self.data = self.splits[n]\n",
    "\n",
    "\n",
    "    def _prepare(self):\n",
    "        _ensure_exists(self.data_path)\n",
    "        for i, item in tqdm(enumerate(self.all_data), total=self.all_data_len,):\n",
    "            with open(\n",
    "                    os.path.join(self.data_path, f\"{self.prefix}_{i}.pkl\"), \"wb\"\n",
    "            ) as file:\n",
    "                pickle.dump(\n",
    "                    (\n",
    "                        torch.tensor(\n",
    "                            [\n",
    "                                self.model[word]\n",
    "                                for word in preprocess_text(item[0], self.stemmer)\n",
    "                            ]\n",
    "                        ),\n",
    "                        item[1],\n",
    "                        item[2],\n",
    "                    ),\n",
    "                    file,\n",
    "                )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx = self.data[idx]\n",
    "        with open(\n",
    "                os.path.join(self.data_path, f\"{self.prefix}_{idx}.pkl\"), \"rb\"\n",
    "        ) as file:\n",
    "            return pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    \"fasttext_crawl\":{\"model\": fasttext.load_model(\"models/cc.en.300.bin\")}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tv.datasets.ImageFolder(f\"{DATA_PATH}images\")\n",
    "\n",
    "data_transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    T.Resize(size=(180, 240)),\n",
    "])\n",
    "dataset.transform = data_transform\n",
    "\n",
    "image_data_loader = torch.utils.data.DataLoader(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data = []\n",
    "\n",
    "# for i, (image, label) in enumerate(tqdm(image_data_loader, 0)):\n",
    "#     filename = image_data_loader.dataset.samples[i][0].split('/')[-1].replace('.jpg','')\n",
    "#     all_data.append([\n",
    "#         df[df[\"video_id\"] == filename][\"title\"].iloc[0],\n",
    "#         i,\n",
    "#         df[df[\"video_id\"] == filename][\"view_count\"].iloc[0]\n",
    "#     ])\n",
    "# with open(\"all_data.pkl\", \"wb\") as file:\n",
    "#     pickle.dump(all_data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"all_data.pkl\", \"rb\") as file:\n",
    "    all_data = pickle.load(file)\n",
    "    \n",
    "# all_data = all_data[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "for model_name in models:\n",
    "    models[model_name][\"train\"] = TitleDataset(\n",
    "        all_data, model_name, True, \"data/embeddings\", models[model_name][\"model\"], prefix=\"train\"\n",
    "    )\n",
    "    models[model_name][\"test\"] = TitleDataset(\n",
    "        all_data,\n",
    "        model_name,\n",
    "        False,\n",
    "        \"data/embeddings\",\n",
    "        models[model_name][\"model\"], prefix=\"train\"\n",
    "    )\n",
    "    models[model_name][\"train_dl_lstm\"] = DataLoader(\n",
    "        models[model_name][\"train\"], batch_size=BATCH_SIZE, collate_fn=pad_collate\n",
    "    )\n",
    "    models[model_name][\"test_dl_lstm\"] = DataLoader(\n",
    "        models[model_name][\"test\"], batch_size=BATCH_SIZE, collate_fn=pad_collate\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# for model_name in models:\n",
    "#     models[model_name][\"train\"]._prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6011 (pid 404262), started 0:07:43 ago. (Use '!kill 404262' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-5714213fe35e6b5e\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-5714213fe35e6b5e\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6011;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "log_dir = \"tensorboard_logs\"\n",
    "_ensure_exists(log_dir)\n",
    "\n",
    "writer_tensorboard = SummaryWriter(log_dir)\n",
    "\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir $log_dir --port=6011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/200 [09:32<31:39:39, 572.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss = 0.000 (r2: -2.559)validation loss = 0.001 (r2: -3.241))\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 200\n",
    "\n",
    "models_to_train = [\"fasttext_crawl\"]\n",
    "# models_to_train = [\"fasttext_crawl\"]\n",
    "\n",
    "\n",
    "for model_name in models_to_train:\n",
    "    kwargs = {\n",
    "        \"num_classes\": 1,\n",
    "        \"input_size\": 300,\n",
    "        \"hidden_size\": 128,\n",
    "        \"num_layers\": 2,\n",
    "        \"image_data_loader\": image_data_loader\n",
    "    }\n",
    "    learning_rate = 0.0001\n",
    "\n",
    "    model_type_name = \"model_lstm\"\n",
    "    dataloader_train_name = \"train_dl_lstm\"\n",
    "    dataloader_test_name = \"test_dl_lstm\"\n",
    "\n",
    "    time_stamp = datetime.now().strftime(\"%d_%m_%y_%H_%M_%S\")\n",
    "    models[model_name][model_type_name] = LSTM(kwargs).cuda()\n",
    "    models[model_name][\"loss_fn\"] = torch.nn.MSELoss(reduction=\"sum\")\n",
    "    models[model_name][\"optimizer\"] = torch.optim.Adam(\n",
    "        models[model_name][model_type_name].parameters(), lr=learning_rate\n",
    "    )\n",
    "\n",
    "    result = fit(\n",
    "        model=models[model_name][model_type_name],\n",
    "        optimizer=models[model_name][\"optimizer\"],\n",
    "        loss_fn=models[model_name][\"loss_fn\"],\n",
    "        train_dl=models[model_name][dataloader_train_name],\n",
    "        val_dl=models[model_name][dataloader_test_name],\n",
    "        writer=writer_tensorboard,\n",
    "        epochs=EPOCHS,\n",
    "        patience=30,\n",
    "        run_prefix=model_name + \"_\" + model_type_name + \"_\" + time_stamp,\n",
    "        print_metrics=True,\n",
    "    )\n",
    "\n",
    "    checkpoint = torch.load(\n",
    "        f\"data/checkpoints/best_{model_name}_{model_type_name}_{time_stamp}\"\n",
    "    )\n",
    "    models[model_name][model_type_name].load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    models[model_name][\"optimizer\"].load_state_dict(checkpoint[\"optimizer_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (resnet): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Identity()\n",
       "  )\n",
       "  (lstm_title): LSTM(300, 128, num_layers=2, batch_first=True)\n",
       "  (fc_1): Linear(in_features=2176, out_features=128, bias=True)\n",
       "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models[model_name][model_type_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"fasttext_crawl\"\n",
    "model_type_name = \"model_lstm\"\n",
    "time_stamp = \"18_01_22_23_47_41\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for LSTM:\n\tMissing key(s) in state_dict: \"resnet.layer1.0.conv3.weight\", \"resnet.layer1.0.bn3.weight\", \"resnet.layer1.0.bn3.bias\", \"resnet.layer1.0.bn3.running_mean\", \"resnet.layer1.0.bn3.running_var\", \"resnet.layer1.0.downsample.0.weight\", \"resnet.layer1.0.downsample.1.weight\", \"resnet.layer1.0.downsample.1.bias\", \"resnet.layer1.0.downsample.1.running_mean\", \"resnet.layer1.0.downsample.1.running_var\", \"resnet.layer1.1.conv3.weight\", \"resnet.layer1.1.bn3.weight\", \"resnet.layer1.1.bn3.bias\", \"resnet.layer1.1.bn3.running_mean\", \"resnet.layer1.1.bn3.running_var\", \"resnet.layer1.2.conv1.weight\", \"resnet.layer1.2.bn1.weight\", \"resnet.layer1.2.bn1.bias\", \"resnet.layer1.2.bn1.running_mean\", \"resnet.layer1.2.bn1.running_var\", \"resnet.layer1.2.conv2.weight\", \"resnet.layer1.2.bn2.weight\", \"resnet.layer1.2.bn2.bias\", \"resnet.layer1.2.bn2.running_mean\", \"resnet.layer1.2.bn2.running_var\", \"resnet.layer1.2.conv3.weight\", \"resnet.layer1.2.bn3.weight\", \"resnet.layer1.2.bn3.bias\", \"resnet.layer1.2.bn3.running_mean\", \"resnet.layer1.2.bn3.running_var\", \"resnet.layer2.0.conv3.weight\", \"resnet.layer2.0.bn3.weight\", \"resnet.layer2.0.bn3.bias\", \"resnet.layer2.0.bn3.running_mean\", \"resnet.layer2.0.bn3.running_var\", \"resnet.layer2.1.conv3.weight\", \"resnet.layer2.1.bn3.weight\", \"resnet.layer2.1.bn3.bias\", \"resnet.layer2.1.bn3.running_mean\", \"resnet.layer2.1.bn3.running_var\", \"resnet.layer2.2.conv1.weight\", \"resnet.layer2.2.bn1.weight\", \"resnet.layer2.2.bn1.bias\", \"resnet.layer2.2.bn1.running_mean\", \"resnet.layer2.2.bn1.running_var\", \"resnet.layer2.2.conv2.weight\", \"resnet.layer2.2.bn2.weight\", \"resnet.layer2.2.bn2.bias\", \"resnet.layer2.2.bn2.running_mean\", \"resnet.layer2.2.bn2.running_var\", \"resnet.layer2.2.conv3.weight\", \"resnet.layer2.2.bn3.weight\", \"resnet.layer2.2.bn3.bias\", \"resnet.layer2.2.bn3.running_mean\", \"resnet.layer2.2.bn3.running_var\", \"resnet.layer2.3.conv1.weight\", \"resnet.layer2.3.bn1.weight\", \"resnet.layer2.3.bn1.bias\", \"resnet.layer2.3.bn1.running_mean\", \"resnet.layer2.3.bn1.running_var\", \"resnet.layer2.3.conv2.weight\", \"resnet.layer2.3.bn2.weight\", \"resnet.layer2.3.bn2.bias\", \"resnet.layer2.3.bn2.running_mean\", \"resnet.layer2.3.bn2.running_var\", \"resnet.layer2.3.conv3.weight\", \"resnet.layer2.3.bn3.weight\", \"resnet.layer2.3.bn3.bias\", \"resnet.layer2.3.bn3.running_mean\", \"resnet.layer2.3.bn3.running_var\", \"resnet.layer3.0.conv3.weight\", \"resnet.layer3.0.bn3.weight\", \"resnet.layer3.0.bn3.bias\", \"resnet.layer3.0.bn3.running_mean\", \"resnet.layer3.0.bn3.running_var\", \"resnet.layer3.1.conv3.weight\", \"resnet.layer3.1.bn3.weight\", \"resnet.layer3.1.bn3.bias\", \"resnet.layer3.1.bn3.running_mean\", \"resnet.layer3.1.bn3.running_var\", \"resnet.layer3.2.conv1.weight\", \"resnet.layer3.2.bn1.weight\", \"resnet.layer3.2.bn1.bias\", \"resnet.layer3.2.bn1.running_mean\", \"resnet.layer3.2.bn1.running_var\", \"resnet.layer3.2.conv2.weight\", \"resnet.layer3.2.bn2.weight\", \"resnet.layer3.2.bn2.bias\", \"resnet.layer3.2.bn2.running_mean\", \"resnet.layer3.2.bn2.running_var\", \"resnet.layer3.2.conv3.weight\", \"resnet.layer3.2.bn3.weight\", \"resnet.layer3.2.bn3.bias\", \"resnet.layer3.2.bn3.running_mean\", \"resnet.layer3.2.bn3.running_var\", \"resnet.layer3.3.conv1.weight\", \"resnet.layer3.3.bn1.weight\", \"resnet.layer3.3.bn1.bias\", \"resnet.layer3.3.bn1.running_mean\", \"resnet.layer3.3.bn1.running_var\", \"resnet.layer3.3.conv2.weight\", \"resnet.layer3.3.bn2.weight\", \"resnet.layer3.3.bn2.bias\", \"resnet.layer3.3.bn2.running_mean\", \"resnet.layer3.3.bn2.running_var\", \"resnet.layer3.3.conv3.weight\", \"resnet.layer3.3.bn3.weight\", \"resnet.layer3.3.bn3.bias\", \"resnet.layer3.3.bn3.running_mean\", \"resnet.layer3.3.bn3.running_var\", \"resnet.layer3.4.conv1.weight\", \"resnet.layer3.4.bn1.weight\", \"resnet.layer3.4.bn1.bias\", \"resnet.layer3.4.bn1.running_mean\", \"resnet.layer3.4.bn1.running_var\", \"resnet.layer3.4.conv2.weight\", \"resnet.layer3.4.bn2.weight\", \"resnet.layer3.4.bn2.bias\", \"resnet.layer3.4.bn2.running_mean\", \"resnet.layer3.4.bn2.running_var\", \"resnet.layer3.4.conv3.weight\", \"resnet.layer3.4.bn3.weight\", \"resnet.layer3.4.bn3.bias\", \"resnet.layer3.4.bn3.running_mean\", \"resnet.layer3.4.bn3.running_var\", \"resnet.layer3.5.conv1.weight\", \"resnet.layer3.5.bn1.weight\", \"resnet.layer3.5.bn1.bias\", \"resnet.layer3.5.bn1.running_mean\", \"resnet.layer3.5.bn1.running_var\", \"resnet.layer3.5.conv2.weight\", \"resnet.layer3.5.bn2.weight\", \"resnet.layer3.5.bn2.bias\", \"resnet.layer3.5.bn2.running_mean\", \"resnet.layer3.5.bn2.running_var\", \"resnet.layer3.5.conv3.weight\", \"resnet.layer3.5.bn3.weight\", \"resnet.layer3.5.bn3.bias\", \"resnet.layer3.5.bn3.running_mean\", \"resnet.layer3.5.bn3.running_var\", \"resnet.layer4.0.conv3.weight\", \"resnet.layer4.0.bn3.weight\", \"resnet.layer4.0.bn3.bias\", \"resnet.layer4.0.bn3.running_mean\", \"resnet.layer4.0.bn3.running_var\", \"resnet.layer4.1.conv3.weight\", \"resnet.layer4.1.bn3.weight\", \"resnet.layer4.1.bn3.bias\", \"resnet.layer4.1.bn3.running_mean\", \"resnet.layer4.1.bn3.running_var\", \"resnet.layer4.2.conv1.weight\", \"resnet.layer4.2.bn1.weight\", \"resnet.layer4.2.bn1.bias\", \"resnet.layer4.2.bn1.running_mean\", \"resnet.layer4.2.bn1.running_var\", \"resnet.layer4.2.conv2.weight\", \"resnet.layer4.2.bn2.weight\", \"resnet.layer4.2.bn2.bias\", \"resnet.layer4.2.bn2.running_mean\", \"resnet.layer4.2.bn2.running_var\", \"resnet.layer4.2.conv3.weight\", \"resnet.layer4.2.bn3.weight\", \"resnet.layer4.2.bn3.bias\", \"resnet.layer4.2.bn3.running_mean\", \"resnet.layer4.2.bn3.running_var\". \n\tsize mismatch for resnet.layer1.0.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n\tsize mismatch for resnet.layer1.1.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n\tsize mismatch for resnet.layer2.0.conv1.weight: copying a param with shape torch.Size([128, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 256, 1, 1]).\n\tsize mismatch for resnet.layer2.0.downsample.0.weight: copying a param with shape torch.Size([128, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([512, 256, 1, 1]).\n\tsize mismatch for resnet.layer2.0.downsample.1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for resnet.layer2.0.downsample.1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for resnet.layer2.0.downsample.1.running_mean: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for resnet.layer2.0.downsample.1.running_var: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for resnet.layer2.1.conv1.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 512, 1, 1]).\n\tsize mismatch for resnet.layer3.0.conv1.weight: copying a param with shape torch.Size([256, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 512, 1, 1]).\n\tsize mismatch for resnet.layer3.0.downsample.0.weight: copying a param with shape torch.Size([256, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([1024, 512, 1, 1]).\n\tsize mismatch for resnet.layer3.0.downsample.1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for resnet.layer3.0.downsample.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for resnet.layer3.0.downsample.1.running_mean: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for resnet.layer3.0.downsample.1.running_var: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for resnet.layer3.1.conv1.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 1024, 1, 1]).\n\tsize mismatch for resnet.layer4.0.conv1.weight: copying a param with shape torch.Size([512, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 1024, 1, 1]).\n\tsize mismatch for resnet.layer4.0.downsample.0.weight: copying a param with shape torch.Size([512, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([2048, 1024, 1, 1]).\n\tsize mismatch for resnet.layer4.0.downsample.1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for resnet.layer4.0.downsample.1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for resnet.layer4.0.downsample.1.running_mean: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for resnet.layer4.0.downsample.1.running_var: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for resnet.layer4.1.conv1.weight: copying a param with shape torch.Size([512, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 2048, 1, 1]).\n\tsize mismatch for fc_1.weight: copying a param with shape torch.Size([128, 640]) from checkpoint, the shape in current model is torch.Size([128, 2176]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-238abaefc0f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34mf\"data/checkpoints/best_{model_name}_{model_type_name}_{time_stamp}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m )\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_type_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_state_dict\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"optimizer\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"optimizer_state_dict\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1481\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1482\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   1483\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   1484\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for LSTM:\n\tMissing key(s) in state_dict: \"resnet.layer1.0.conv3.weight\", \"resnet.layer1.0.bn3.weight\", \"resnet.layer1.0.bn3.bias\", \"resnet.layer1.0.bn3.running_mean\", \"resnet.layer1.0.bn3.running_var\", \"resnet.layer1.0.downsample.0.weight\", \"resnet.layer1.0.downsample.1.weight\", \"resnet.layer1.0.downsample.1.bias\", \"resnet.layer1.0.downsample.1.running_mean\", \"resnet.layer1.0.downsample.1.running_var\", \"resnet.layer1.1.conv3.weight\", \"resnet.layer1.1.bn3.weight\", \"resnet.layer1.1.bn3.bias\", \"resnet.layer1.1.bn3.running_mean\", \"resnet.layer1.1.bn3.running_var\", \"resnet.layer1.2.conv1.weight\", \"resnet.layer1.2.bn1.weight\", \"resnet.layer1.2.bn1.bias\", \"resnet.layer1.2.bn1.running_mean\", \"resnet.layer1.2.bn1.running_var\", \"resnet.layer1.2.conv2.weight\", \"resnet.layer1.2.bn2.weight\", \"resnet.layer1.2.bn2.bias\", \"resnet.layer1.2.bn2.running_mean\", \"resnet.layer1.2.bn2.running_var\", \"resnet.layer1.2.conv3.weight\", \"resnet.layer1.2.bn3.weight\", \"resnet.layer1.2.bn3.bias\", \"resnet.layer1.2.bn3.running_mean\", \"resnet.layer1.2.bn3.running_var\", \"resnet.layer2.0.conv3.weight\", \"resnet.layer2.0.bn3.weight\", \"resnet.layer2.0.bn3.bias\", \"resnet.layer2.0.bn3.running_mean\", \"resnet.layer2.0.bn3.running_var\", \"resnet.layer2.1.conv3.weight\", \"resnet.layer2.1.bn3.weight\", \"resnet.layer2.1.bn3.bias\", \"resnet.layer2.1.bn3.running_mean\", \"resnet.layer2.1.bn3.running_var\", \"resnet.layer2.2.conv1.weight\", \"resnet.layer2.2.bn1.weight\", \"resnet.layer2.2.bn1.bias\", \"resnet.layer2.2.bn1.running_mean\", \"resnet.layer2.2.bn1.running_var\", \"resnet.layer2.2.conv2.weight\", \"resnet.layer2.2.bn2.weight\", \"resnet.layer2.2.bn2.bias\", \"resnet.layer2.2.bn2.running_mean\", \"resnet.layer2.2.bn2.running_var\", \"resnet.layer2.2.conv3.weight\", \"resnet.layer2.2.bn3.weight\", \"resnet.layer2.2.bn3.bias\", \"resnet.layer2.2.bn3.running_mean\", \"resnet.layer2.2.bn3.running_var\", \"resnet.layer2.3.conv1.weight\", \"resnet.layer2.3.bn1.weight\", \"resnet.layer2.3.bn1.bias\", \"resnet.layer2.3.bn1.running_mean\", \"resnet.layer2.3.bn1.running_var\", \"resnet.layer2.3.conv2.weight\", \"resnet.layer2.3.bn2.weight\", \"resnet.layer2.3.bn2.bias\", \"resnet.layer2.3.bn2.running_mean\", \"resnet.layer2.3.bn2.running_var\", \"resnet.layer2.3.conv3.weight\", \"resnet.layer2.3.bn3.weight\", \"resnet.layer2.3.bn3.bias\", \"resnet.layer2.3.bn3.running_mean\", \"resnet.layer2.3.bn3.running_var\", \"resnet.layer3.0.conv3.weight\", \"resnet.layer3.0.bn3.weight\", \"resnet.layer3.0.bn3.bias\", \"resnet.layer3.0.bn3.running_mean\", \"resnet.layer3.0.bn3.running_var\", \"resnet.layer3.1.conv3.weight\", \"resnet.layer3.1.bn3.weight\", \"resnet.layer3.1.bn3.bias\", \"resnet.layer3.1.bn3.running_mean\", \"resnet.layer3.1.bn3.running_var\", \"resnet.layer3.2.conv1.weight\", \"resnet.layer3.2.bn1.weight\", \"resnet.layer3.2.bn1.bias\", \"resnet.layer3.2.bn1.running_mean\", \"resnet.layer3.2.bn1.running_var\", \"resnet.layer3.2.conv2.weight\", \"resnet.layer3.2.bn2.weight\", \"resnet.layer3.2.bn2.bias\", \"resnet.layer3.2.bn2.running_mean\", \"resnet.layer3.2.bn2.running_var\", \"resnet.layer3.2.conv3.weight\", \"resnet.layer3.2.bn3.weight\", \"resnet.layer3.2.bn3.bias\", \"resnet.layer3.2.bn3.running_mean\", \"resnet.layer3.2.bn3.running_var\", \"resnet.layer3.3.conv1.weight\", \"resnet.layer3.3.bn1.weight\", \"resnet.layer3.3.bn1.bias\", \"resnet.layer3.3.bn1.running_mean\", \"resnet.layer3.3.bn1.running_var\", \"resnet.layer3.3.conv2.weight\", \"resnet.layer3.3.bn2.weight\", \"resnet.layer3.3.bn2.bias\", \"resnet.layer3.3.bn2.running_mean\", \"resnet.layer3.3.bn2.running_var\", \"resnet.layer3.3.conv3.weight\", \"resnet.layer3.3.bn3.weight\", \"resnet.layer3.3.bn3.bias\", \"resnet.layer3.3.bn3.running_mean\", \"resnet.layer3.3.bn3.running_var\", \"resnet.layer3.4.conv1.weight\", \"resnet.layer3.4.bn1.weight\", \"resnet.layer3.4.bn1.bias\", \"resnet.layer3.4.bn1.running_mean\", \"resnet.layer3.4.bn1.running_var\", \"resnet.layer3.4.conv2.weight\", \"resnet.layer3.4.bn2.weight\", \"resnet.layer3.4.bn2.bias\", \"resnet.layer3.4.bn2.running_mean\", \"resnet.layer3.4.bn2.running_var\", \"resnet.layer3.4.conv3.weight\", \"resnet.layer3.4.bn3.weight\", \"resnet.layer3.4.bn3.bias\", \"resnet.layer3.4.bn3.running_mean\", \"resnet.layer3.4.bn3.running_var\", \"resnet.layer3.5.conv1.weight\", \"resnet.layer3.5.bn1.weight\", \"resnet.layer3.5.bn1.bias\", \"resnet.layer3.5.bn1.running_mean\", \"resnet.layer3.5.bn1.running_var\", \"resnet.layer3.5.conv2.weight\", \"resnet.layer3.5.bn2.weight\", \"resnet.layer3.5.bn2.bias\", \"resnet.layer3.5.bn2.running_mean\", \"resnet.layer3.5.bn2.running_var\", \"resnet.layer3.5.conv3.weight\", \"resnet.layer3.5.bn3.weight\", \"resnet.layer3.5.bn3.bias\", \"resnet.layer3.5.bn3.running_mean\", \"resnet.layer3.5.bn3.running_var\", \"resnet.layer4.0.conv3.weight\", \"resnet.layer4.0.bn3.weight\", \"resnet.layer4.0.bn3.bias\", \"resnet.layer4.0.bn3.running_mean\", \"resnet.layer4.0.bn3.running_var\", \"resnet.layer4.1.conv3.weight\", \"resnet.layer4.1.bn3.weight\", \"resnet.layer4.1.bn3.bias\", \"resnet.layer4.1.bn3.running_mean\", \"resnet.layer4.1.bn3.running_var\", \"resnet.layer4.2.conv1.weight\", \"resnet.layer4.2.bn1.weight\", \"resnet.layer4.2.bn1.bias\", \"resnet.layer4.2.bn1.running_mean\", \"resnet.layer4.2.bn1.running_var\", \"resnet.layer4.2.conv2.weight\", \"resnet.layer4.2.bn2.weight\", \"resnet.layer4.2.bn2.bias\", \"resnet.layer4.2.bn2.running_mean\", \"resnet.layer4.2.bn2.running_var\", \"resnet.layer4.2.conv3.weight\", \"resnet.layer4.2.bn3.weight\", \"resnet.layer4.2.bn3.bias\", \"resnet.layer4.2.bn3.running_mean\", \"resnet.layer4.2.bn3.running_var\". \n\tsize mismatch for resnet.layer1.0.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n\tsize mismatch for resnet.layer1.1.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n\tsize mismatch for resnet.layer2.0.conv1.weight: copying a param with shape torch.Size([128, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 256, 1, 1]).\n\tsize mismatch for resnet.layer2.0.downsample.0.weight: copying a param with shape torch.Size([128, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([512, 256, 1, 1]).\n\tsize mismatch for resnet.layer2.0.downsample.1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for resnet.layer2.0.downsample.1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for resnet.layer2.0.downsample.1.running_mean: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for resnet.layer2.0.downsample.1.running_var: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for resnet.layer2.1.conv1.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 512, 1, 1]).\n\tsize mismatch for resnet.layer3.0.conv1.weight: copying a param with shape torch.Size([256, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 512, 1, 1]).\n\tsize mismatch for resnet.layer3.0.downsample.0.weight: copying a param with shape torch.Size([256, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([1024, 512, 1, 1]).\n\tsize mismatch for resnet.layer3.0.downsample.1.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for resnet.layer3.0.downsample.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for resnet.layer3.0.downsample.1.running_mean: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for resnet.layer3.0.downsample.1.running_var: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for resnet.layer3.1.conv1.weight: copying a param with shape torch.Size([256, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 1024, 1, 1]).\n\tsize mismatch for resnet.layer4.0.conv1.weight: copying a param with shape torch.Size([512, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 1024, 1, 1]).\n\tsize mismatch for resnet.layer4.0.downsample.0.weight: copying a param with shape torch.Size([512, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([2048, 1024, 1, 1]).\n\tsize mismatch for resnet.layer4.0.downsample.1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for resnet.layer4.0.downsample.1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for resnet.layer4.0.downsample.1.running_mean: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for resnet.layer4.0.downsample.1.running_var: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for resnet.layer4.1.conv1.weight: copying a param with shape torch.Size([512, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 2048, 1, 1]).\n\tsize mismatch for fc_1.weight: copying a param with shape torch.Size([128, 640]) from checkpoint, the shape in current model is torch.Size([128, 2176])."
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load(\n",
    "    f\"data/checkpoints/best_{model_name}_{model_type_name}_{time_stamp}\"\n",
    ")\n",
    "models[model_name][model_type_name].load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "models[model_name][\"optimizer\"].load_state_dict(checkpoint[\"optimizer_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "text, image, y= next(iter(models[\"fasttext_crawl\"][dataloader_test_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\n",
    "    f\"models/BestModel18\"\n",
    ")\n",
    "models[model_name][model_type_name].load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "models[model_name][\"optimizer\"].load_state_dict(checkpoint[\"optimizer_state_dict\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 18_01_22_23_47_41\" resnet18, layers: 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
